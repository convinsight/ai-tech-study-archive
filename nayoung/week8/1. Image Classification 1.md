# Image Classification 1

### **강의 소개**

우리는 오감 중 특히 시각에 의존하여 사물을 바라보고 이해하며 살아가고 있습니다.동일한 프로세스를 컴퓨터에 적용한 컴퓨터 비전입니다. 본 강의에서는 컴퓨터 비전 (CV)의 첫 시간으로 CV에 대해 짧게 소개하고, CV에서 가장 기본적인 task, image clasiification을 소개합니다. 

Image Classification은 사진이 주어졌을 때  특정 카테고리로 분류하는 task입니다. 이번 강의에서는 먼저 기존의 머신러닝과 구분되는 딥러닝을 사용한 Image classification의 특징에 대해서 배웁니다. 다음으로 대표적인 CNN 모델인 AlexNet을 배우고 이에 대한 실습을 진행합니다. 끝으로 가장 유명한 classification 모델 중 하나인 VGGNet에 대해 배웁니다.

### **Further Reading**

- VGGNet : [https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf)

# Course overview

## Why is visual perception important?

### Artificial Intelligence (AI)

```jsx
The theory and development of computer systems able to perform tasks normally 
requiring human intelligence, such as visual perception, speech recognition, 
decision-making, and translation between languages. 
	--from the Oxford dictionary
```

### Perception to system?

- Developing machine perception is still open research area

### Why is visual perception important?

- Understanding
    - 뇌 영역의 50% 이상 → visual information 처리
- Sensing
    - 정보의 75%는 눈으로부터
    

## What is computer vision?

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/f76c77b7-09dc-4eee-8650-21aab17366ef/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T221821Z&X-Amz-Expires=86400&X-Amz-Signature=990002a27229198cd2b1bcbff339758be0996323b64b720685760815a06badfd&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/7dca0623-a019-4e28-a821-8713c04b87eb/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T221928Z&X-Amz-Expires=86400&X-Amz-Signature=a14f8e49cba543e927a5f8b68ce1d72db937ec2c374ed7df45f57341814a91cd&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

- Rendering → 정보를 통해 2D 이미지 표현

### Visual perception & intelligence

- Input : visual data (image or video)

### Class of visual perception

- Color perception
- Motion perception
- 3D perception
- Semantic-level perception
- Social perception (emotion perception)
- Visuomotor perception

### Our visual perception is imperfect

- 사람도 똑바로 된 이미지를 ‘많이’ 봤기(= bias) 때문에 거꾸로 된 이미지를 구별 가능

### How to implement?

- Machine Learning
    - 사람이 feature extraction

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/a8579dee-9d69-42f8-a747-30cc3d7553a2/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222012Z&X-Amz-Expires=86400&X-Amz-Signature=649a31a7dc9f6688eef3eea05f28b81d0e52f9d4ce1b7f8a61eb34870e5b9d89&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

- Deep Learning
<details>
<summary>📎feature extraction  </summary>
<div markdown="1">       

   - **원본 특징 들의 조합으로 새로운 특징을 생성하는 것이다.**
   - 고차원의 원본 feature 공간을 저차원의 새로운 feature 공간으로 투영시킨다. 새롭게 구성된 feature 공간은 보통은 원본 feature 공간의 선형 또는 비선형 결합이다.
   - 가장 대표적인 알고리즘으로 PCA(Principle Component Analysis)가 있다. PCA를 간단히 설명하면 각 변수(Feature)를 하나의 축으로 투영시켰을 때 분산이 가장 큰 축을 첫번째 주성분으로 선택하고 그 다음 큰 축을 두번째 주성분으로 선택하고 **데이터를 선형 변환하여 다차원을 축소하는 방법이다.**
   - 참고 : [기계학습/feature engineering - 인코덤, 생물정보 전문위키 (incodom.kr)](http://www.incodom.kr/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/feature_engineering#h_2c113344b39c04c8599b84f8fa93718c)
   - 읽어보기 : [Feature Extraction Techniques. An end to end guide on how to reduce a… | by Pier Paolo Ippolito | Towards Data Science](https://towardsdatascience.com/feature-extraction-techniques-d619b56e31be)

</div>
</details> 
  
  
<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/1070890a-ee6a-4123-be3f-009b91c5f8ea/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222039Z&X-Amz-Expires=86400&X-Amz-Signature=a23f1f5e95bd34d0535dea05c5ed888a474f78b32b7cbd6afd88d0404363f3d4&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

## What you will learn in this course

### Fundamental image tasks

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/a2a0196e-87c8-423d-9093-7f94ef5fa5a3/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222111Z&X-Amz-Expires=86400&X-Amz-Signature=fd9aa904c6098fe636b327da7ddb7d1c8e9a7eff8196b4e028308568761b7b88&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

### Data augmentation and knowledge distillation

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/643afaea-e0aa-49c7-bb93-6d9db1398f22/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222131Z&X-Amz-Expires=86400&X-Amz-Signature=9c5806fb4cefdac419b28a19ea6dd16dd50dd8f577fbd92f3e9774fab4a3adec&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

<details>
<summary>📎 knowledge distillation </summary>
<div markdown="1">    
  
- 크고 무거운 모델의 정보(knowledge)를 작고 가벼운 모델로 전달하여 작고 가벼운 모델이 더 정확한 추론을 하도록 학습시키는 방법론
- Knowledge distillation 의 목적은 "미리 잘 학습된 큰 네트워크(Teacher network)의 지식을 실제로 사용하고자 하는 **작은 네트워크(Student network)** 에게 전달하는 것" 입니다.
- 참고 : [1. Knowledge Distillation이란? :: Time Traveler (tistory.com)](https://89douner.tistory.com/143)
- 참고 : [딥러닝 용어 정리, Knowledge distillation 설명과 이해 (tistory.com)](https://light-tree.tistory.com/196)

</div>
</details>


### Multi-modal learning (vision + {text, sound, 3D})

- 다른 perception과 vision을 함께 학습

### Conditional generative model

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/41c7bf64-2074-4fc8-9ae9-c18dee75552c/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222154Z&X-Amz-Expires=86400&X-Amz-Signature=5d517a0490dc42d2e6135c5530d6f2527b8ee10f7108647decff5de763c6e8f1&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

### Neural network analysis by visualization

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/d02aa956-c9d7-4f38-9f7b-a86602bf42e3/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222216Z&X-Amz-Expires=86400&X-Amz-Signature=f68d31d558d856345d7cb421c3c60282b5b562393e1e9610691b4e13e6e27d27&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

# Image classification

## What is classification?

### Classifier

- mapping function that maps an image to a **category level**

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/26c4b61b-c6ac-48fd-97c3-53ee0884de80/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222234Z&X-Amz-Expires=86400&X-Amz-Signature=1f33520cc82d4b5de8de68be29012c1fd6977f6c8519b62cfdf4d7301cabddd4&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

## An ideal approach for image recognition

- 세상의 모든 이미지를 memorize → classification 문제를 k Nearest Neighbors (k-NN)으로 해결
- but, 불가능
    - ∵ 모든 데이터를 저장하는 것이 불가능
    - ∵ k-NN하려면 데이터(이미지)간의 유사도를 정의해야 → 유사도 정의 hard
    - ∵ Time complexity (ex. linear search) → O(n) (n=infinite)
    - ∵ Memory complexity → O(n) (n=infinite)
<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bfe58c1a-6c5f-456d-ab8a-20b68b13f2ff/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222252Z&X-Amz-Expires=86400&X-Amz-Signature=8905b0dbcd837c0731c94a9384073e98e5eb7026b2794225a123d51c5be2479e&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

### k Nearest Neighbors (k-NN)

- query data 근방의 k개의 data를 보고 query data를 classify

<img width="30%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/c08938b7-7371-47e7-9984-f33f8fb3e300/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222312Z&X-Amz-Expires=86400&X-Amz-Signature=83064e7c7e5d41838f796f79aecba4a8fa0542b85e453bda4f10549a72ccde58&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

<details>
<summary>📎k-NN  </summary>
<div markdown="1">       
  
- 새로운 데이터가 입력되었을 때, 기존의 데이터와 새로운 데이터를 비교함으로써 새로운 데이터와 가장 인접한 데이터 k개를 선정한다. 이어서, k값에 의해 결정된 분류를 입력된 데이터의 분류로 확정한다. 즉, 새로 입력된 데이터와 기존 데이터를 비교함으로써 새로운 데이터를 유사하게 판단된 기존 데이터로 분류한다.    
- cf) k는 보통 홀수를 많이 사용    
- 참고 : [kNN(k Nearest Neighbor) 알고리즘 (tistory.com)](https://computer-science-student.tistory.com/56)  

</div>
</details>


## Convolutional Neural Networks (CNN)

- 모든 data를 neural network에 compress

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/b47458e5-5cee-4e22-a729-a5ab76a2d793/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222332Z&X-Amz-Expires=86400&X-Amz-Signature=064c0f98e4f7ab4527ced88123c4f2762d87ed61907479624eecc6310c7b8f2b&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

### visualization of single fully connected layer networks

<details>
<summary>📎fully connected layer  </summary>
<div markdown="1">     
  
- 한층의 모든 뉴런이 다음층이 모든 뉴런과 연결된 상태로, 2차원의 배열 형태 이미지를 1차원의 평탄화 작업을 통해 이미지를 분류하는데 사용되는 계층입니다.   
      1. 2차원 배열 형태의 이미지를 1차원 배열로 평탄화   
      2. 활성화 함수(Relu, Leaky Relu, Tanh,등)뉴런을 활성화   
      3. 분류기(Softmax) 함수로 분류    
- 참고 + 읽어보기 : [[딥러닝 레이어] FC(Fully Connected Layers)이란? : 네이버 블로그 (naver.com)](https://blog.naver.com/PostView.nhn?blogId=intelliz&logNo=221709190464)

</div>
</details>

<details>
<summary>📎FNN VS. CNN </summary>
<div markdown="1">      
  
- CNN이 나오기 이전 이미지 인식은 2차원으로 이미지(채널까지 포함 3차원)를 1차원 배열로 바꾼 뒤 **FNN (Fully- connected multi layered Neural Network)** 신경망으로 학습시키는 방법 이었다.
- FNN의 문제점은 인접 픽셀간의 상관관계가 무시된다는 것이다. FNN은 벡터 형태로 표현된 데이터를 입력 받기 때문에 이미지를 반드시 벡터화 해야 한다. 그러나 이미지 데이터는 일반적으로 인접한 픽셀간의 상관관계가 매우 높기 때문에 이미지를 **벡터화 (vectorization)하는 과정에서 정보 손실이 발생한다.**
<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/8ce7dfb5-1da3-474b-8ade-21fefc842967/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222503Z&X-Amz-Expires=86400&X-Amz-Signature=bfa32b8dab07d44b45ada0de019f236a212898cf696f87ee953c8691b20cc7e1&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">


- CNN은 이미지의 형태를 보존하도록 행렬 형태의 데이터를 입력 받기 때문에 이미지를 벡터화 하는 과정에서 발생하는 **정보 손실을 방지**할 수 있다.
- 참고 : [CNN (Convolutional Neural Network) 개념 : 네이버 블로그 (naver.com)](https://m.blog.naver.com/jevida/221841296542)

</div>
</details>


<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/97924849-37f5-40bb-95b6-ebb9010b28fc/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222434Z&X-Amz-Expires=86400&X-Amz-Signature=e305de008399314dbc71123eabe838f1ac507cf9e7b21f9cce690e79bbd2a718&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

- 문제 1️⃣ 평균 영상(이미지) 외에는 표현 불가능

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0adaba2f-bb90-4a9e-981a-48aac96b92ca/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222536Z&X-Amz-Expires=86400&X-Amz-Signature=e625831e9caed20f58852ca9f003a9c5ae81fbe018ca2e0e4a808a0b3baea9b3&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

- 문제 2️⃣ test time 때 문제 발생
    - crop 된 사진이 input으로 들어오면, 이런 패턴은 학습한 적이 없으므로 틀린 output 내놓음

<img width="50%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/1078c352-89bd-4d41-acb2-9ef84ee63ac6/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222552Z&X-Amz-Expires=86400&X-Amz-Signature=960cc5b690c9be9b45317eebd528eadde08048492aeeff4a208a631367b16d12&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

### Convolution neural networks are ~~fully~~ `locally` connected neural networks

<img width="50%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/28f1617d-f8d1-4603-929f-7d19b28301d5/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222608Z&X-Amz-Expires=86400&X-Amz-Signature=4c16461101681192bb36ac52706a1757f4b7e409e085ec179485b09218760c23&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

- Local feature learning
    - feature 1개 얻기 위해서
        - fully connected → 패턴 바뀌면 틀릴 확률 ↑
        - locally connected
- Parameter sharing
    - hidden node(filter) 재활용 가능 → parameter 개수 ↓ + overfitting 방지 (∵ parameter 개수 多 이면, overfitting)

### backbone으로 사용되는 CNN

- CNN → feature extraction 역할
- target network head
    - image-level classification
    - classification + regression → object detection
    - pixel-level classification → segmentation
 
<details> <summary>📎box regression  </summary> <div markdown="1">       

- **predicted box**가 **ground truth box**와 유사하도록 학습하는 것
- 참고 : [Bounding box regression (tistory.com)](https://better-tomorrow.tistory.com/entry/Bounding-box-regression)

</div>
</details>

<img width="50%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/fb17cf64-b720-40c6-ae33-8d202730c250/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222630Z&X-Amz-Expires=86400&X-Amz-Signature=9766ce1c84d2f3ac38cc47b3ffab4c8084fa26f52b8aff2c8298b5e1d1183aa4&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

<details>
<summary>📎backbone  </summary>
<div markdown="1">       
  
<img width="50%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/856b571e-88a1-47de-bcc6-0633e9ffab4a/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222654Z&X-Amz-Expires=86400&X-Amz-Signature=072c1837bdd328e4ce0bf09fd27937d2d2aeea1a35f93c2ca28b77ffa12b6833&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">
  
- Backbone은 등뼈라는 뜻인데, 즉 척추는 뇌와 몸의 각 부위의 신경을 이어주는 역할을 한다.
- 입력: 뇌를 통해, 출력: 팔, 다리 라고 생각하면 backbone은 입력이 처음 들어와서 출력에 관련된 모듈에 처리된 입력을 보내주는 역할이라고 생각할 수 있다.
- 개체를 검출하든 영역들을 나누든 Neural Network는 **입력 이미지로부터 다양한 feature를 추출**해야 한다. 그 역할을 **backbone 네트워크**가 한다.
- 참고 : [딥러닝에서 Backbone Network란? : 네이버 블로그 (naver.com)](https://blog.naver.com/keeping816/221681396990)

</div>
</details>

<details>
<summary>📎classification + regression → object detection  </summary>
<div markdown="1">       
  
  
- convolution network를 통해 **classification** + box **regression**(localization)을 수행한다
- 참고 : [[AI/딥러닝] 진정한 딥러닝을 위한 3가지 분류 (Classification, Object Detection, Image Segmentation) 2탄 (tistory.com)](https://rubber-tree.tistory.com/133?category=966437)


</div>
</details>
    

# CNN architectures for image classification 1

## History

<img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/618230e5-2a36-4e71-86e7-43f03c14ebae/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222711Z&X-Amz-Expires=86400&X-Amz-Signature=14de7ada60f86a4f0d1c7354fb3f1bae10f65013a5d9bb47299fe300555a2b1b&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

## AlexNet

### LeNet-5

- by Yann LeCun
- `Conv` - `Pool` - `Conv` - `Pool` - `FC` - `FC`
- Convolution : 5x5 filters with stride 1
- Pooling : 2x2 max pooling with stride 2
<details>
<summary>📎 pooling layer </summary>
<div markdown="1">       
  
1. input size를 줄임(Down Sampling).
  - 텐서의 크기를 줄이는 역할을 한다.
2. overfitting을 조절
  - input size가 줄어드는 것은 그만큼 쓸데없는 parameter의 수가 줄어드는 것이라고 생각할 수 있다. 훈련데이터에만 높은 성능을 보이는 과적합(overfitting)을 줄일 수 있다.
3. 특징을 잘 뽑아낸다.
  - pooling을 했을 때, 특정한 모양을 더 잘 인식할 수 있다.
4. 지역적 이동에 노이즈를 줌으로써 일반화 성능을 올려준다.
  - max pooling의 경우 주어진 픽셀중 큰것만 뽑기때문에 모양이 조금 달라지는 특성을 가지고 있다
- 참고 : [[ CNN ] pooling이란? (tf.keras.layers.MaxPool2D) (tistory.com)](https://supermemi.tistory.com/16)

</div>
</details>    


### AlexNet

- **LeNet-5와의 차이점**
    - **bigger** (7 hidden layers + 605k neurons + 60 million parameters)
    - trained with **ImageNet**
    - **ReLU** (activation function) + **dropout** (regularization technique)
- **overall architecture**
    - `Conv` - `Pool` - `LRN` - `Conv` - `Pool` - `LRN` - `Conv` - `Conv` - `Conv` - `Pool` - `FC` - `FC` - `FC`
    
    <img width="70%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e528d672-cdd1-4837-aff0-4fec614c6709/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222728Z&X-Amz-Expires=86400&X-Amz-Signature=8ed4113df27a07b6050b7f7270b6e4d1bc5afb86a33ae5171330e45f6e92ac58&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">
    
    - (위의 그림) GPU 2장으로 나눠서 사용 (단, LRN 사용 O)
        - 중간중간 activation map cross 일어남
        
    <img width="50%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bd5d3e4a-fb85-43d4-99ea-c3eeb8157636/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222758Z&X-Amz-Expires=86400&X-Amz-Signature=3e68970b64c9b99f7af80413f8c0b59255c7580a3492f1d7db61c5c5715d3c54&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">
    
    - (위의 그림) GPU 1장을 사용 (단, LRN 사용 X)
<details>
<summary>📎 activation map </summary>
<div markdown="1">     
  
- 하나의 **convolution filter**가 순차적으로 input 데이터를 거치게 되면, 하나의 **map 형태의 결과값**이 나오게 되는데, 이를 feature map이라고 합니다.
- 이때, feature map에서 activation function을 적용한 결과를 activation map이라고 합니다.
- 참고 : [1. Activation Map :: Time Traveler (tistory.com)](https://89douner.tistory.com/260#:~:text=%ED%95%98%EB%82%98%EC%9D%98%20conv%20filter%EA%B0%80%20%EC%88%9C%EC%B0%A8%EC%A0%81%EC%9C%BC%EB%A1%9C%20input%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC,or%20activation%20map%EC%9D%B4%EB%9D%BC%EA%B3%A0%20%ED%95%A9%EB%8B%88%EB%8B%A4.)
- 참고 : [[Deep Learning] 헷갈리는 기본 용어 모음집 — Constructing Future (tistory.com)](https://jisuhan.tistory.com/34)
    

</div>
</details>   

<details>    
<summary>📎 tensor → vector </summary>
<div markdown="1">       
  
- CNN에서 Convolution Layer와 Pooling Layer를 반복적으로 거치면서 주요 특징만 추출되는데 이 때 추출된 주요 특징은 2차원 데이터로 이루어져 있지만 fc layer(Dense)와 같이 분류를 위한 학습 레이어에서는 1차원 데이터로 바꾸어서 학습이 되어야 한다. **이때 Flatten Layer가 2차원 데이터를 1차원 데이터로 바꾸는 역할을 한다.**

<img width="50%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/3424f94c-066c-433f-9ffe-0650abad4cb2/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222820Z&X-Amz-Expires=86400&X-Amz-Signature=92905a67c19e8123eece5cd19a7195a20299d7bd99cb410f37e6853cec49b084&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

- 참고 : [Tensorflow 2.0 - 각 Layer별 역할 개념 및 파라미터 파악 (tistory.com)](https://jeongminhee99.tistory.com/121)

</div>
</details>


<details>
<summary>📎 average pooling VS. flatten ? </summary>
<div markdown="1">       
  
  
- `Flatten Layer`를 이용해서 입력받은 값을 굉장히 긴 하나의 벡터로 만든 다음, 그 벡터를 FC Layer에 넣는 방식으로 하나하나 매핑해서 클래스를 분류했습니다. 이 과정에서 **공간적 정보도 많이 잃어**버리는데다가, 굉장히 많은 파라미터, 즉 **가중치가 많이 필요**하고, VGGNet의 경우 이 부분이 전체 계산량의 85%를 차지했습니다. 컨볼루션 레이어를 아무리 쌓아도 FC Layer 하나를 못 따라갈 정도입니다.
- `Global Average Pooling layer`는 **분류할 클래스 수만큼 feature map을 마지막에 생성**합니다. 예를 들어, feature map이 6개니까 분류할 클래스 수가 6개라고 가정합시다. 그럼 그 feature map 안에 있는 특징값들의 평균을 구해서 각각의 출력 노드에 바로 입력하는 방식입니다. 여기서는 각 feature map의 평균이 7, 4, 8, 6, 3, 5가 나왔다고 합시다.
        - 즉, 단순히 **i번째 feature map의 평균값을 구해서 i번째 출력 노드에 입력하는 것**
        입니다.  
- GAP의 장점

        1. Location 정보를 FC Layer보다 적게 잃는다.

        2. 파라미터를 차지하지 않아 계산 속도가 빠르다.

        3. 파라미터가 많아지지 않기 때문에 오버피팅을 방지한다.

        4. feature map 안의 값들의 평균을 사용하기 때문에 global context 정보를 가진다.

- 참고 : [GAP (Global Average Pooling) : 전역 평균 풀링 (tistory.com)](https://mole-starseeker.tistory.com/66)

</div>
</details>

        
- **deprecated components**
    - 1️⃣ Local Response Normalization (LRN) → Batch normalization 사용            
    - 2️⃣ 11x11 convolution filter  → 크기가 큰 filter 사용 X
        - The filter size ↑ → the input size of the image ↑
            - LeNet: 28x28
            - AlexNet: 227x227
        - Larger size **filters** are used to cover **a wider range of the input image**
    - 3️⃣ Receptive field in CNN
        - (가정) KxK Conv + stride 1 + PxP pooling layer
            - → input size = (P+K-1) X (P+K-1)
            - 일반적인 conv output layer 구하는 공식과 동일
            - (아래 그림) Layer i (input layer) ↔ Layer i+1 (pooling layer)
            
            <img width="30%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/17be9a27-7150-46b8-96d3-c809e1dae14f/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T222952Z&X-Amz-Expires=86400&X-Amz-Signature=2b96ca4547b8a91b3314c6943475a58c02fdcca07e8cab85df6d40e3043c25de&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

<details>
<summary>📎LRN (local response normalization)  </summary>
<div markdown="1">    
  
- `ReLU`는 양수의 방향으로는 입력의 값을 그대로 사용합니다. 그렇게 되면, Conv나 Pooling 시 매우 높은 하나의 픽셀값이 주변의 픽셀에 영향을 미치게 됩니다. → overfitting 가능 (∵ training data에만 feature가 크게 반응했으므로)
- 이런 부분을 방지(= 한 filter에서만 과도하게 activate 되는 것을 방지)하기 위해서 다른 activation map의 같은 위치에 있는 픽셀끼리 `normalization`을 진행합니다.
- 참고 : [LRN(Local Response Normalization) 이란 무엇인가?(feat. AlexNet) :: Taegu (tistory.com)](https://taeguu.tistory.com/29)
- 참고 : [AlexNet: ImageNet Classification wtih Deep Convolutional Neural Networks Curaai00's Deep Learning Blog (tistory.com)](https://curaai00.tistory.com/4)
- 읽어보기 : [AlexNet (tistory.com)](https://eremo2002.tistory.com/112)

</div>
</details>

<details>
<summary>📎Receptive field  </summary>
<div markdown="1">       
  
- Receptive Field는 **filter가 한번에 보는 영역**으로 생각하면 됩니다. 일반적인 3x3 filter size는 receptive field가 3x3 이미지 입니다. 그리고 이러한 layer를 두개, 세개 쌓으면 receptive filed가 5x5, 7x7 이런식으로 늘어나게 됩니다. Receptive field가 늘어난다는 것은 output을 계산할때 사용하는 정보의 양이 많다는 것 입니다.
- 정보의 양이 늘어나면, 성능이 좋아질 확률도 높아지지만, 학습해야 할 양이 많아서 연산량이 증가하게 되는 단점도 있습니다. 이 **Receptive field를 높이기 위해서 filter의 크기를 키우거나, layer를 늘릴 수 있습니다. 또는 pooling 등을 사용하는 것도 receptive field를 높일 수 있습니다.** Poling의 경우 연산량 까지 감소할 수 있지만 정보의 손실을 가져올 수도 있죠.
- 참고 : [https://dataplay.tistory.com/29](https://dataplay.tistory.com/29)

</div>
</details>

<details>
<summary>📎output layer size 수식  </summary>
<div markdown="1">   
  
<img width="50%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e95d66e4-b64c-4057-ba29-54aebf2930f9/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T223020Z&X-Amz-Expires=86400&X-Amz-Signature=7e198102c71adf82bbf5ee5e148373447979cf430366950e846817f712cfa3e9&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">

- 참고 : [06. 합성곱 신경망 - Convolutional Neural Networks (tistory.com)](https://excelsior-cjh.tistory.com/180)

</div>
</details>


## VGGNet

- Deeper architecture → 16, 19 layers
- Simpler architecture
    - LRN X
    - 3x3 Convolution + 2x2 max pooling (11x11 Convolution X)
- Better performance
    - AlexNet 보다 성능이 좋음 (2nd in ILSVRC14)
- overall architecture

<img align="left" width="25%" src="https://s3.us-west-2.amazonaws.com/secure.notion-static.com/b15c81c5-486f-4d19-af87-6d2aa6abd4d8/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220315%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220315T035753Z&X-Amz-Expires=86400&X-Amz-Signature=ecf5b78e287225c83d445dbf39ee1bb19877c99847014bb18160895cde662895&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject">


`output`

- 3 fully-connected (FC) layers

`Key design choices`

- 3x3 convolution filters with stride 1
- 2x2 max pooling operations

⇒ Using **many** **3x3 conv layers** instead of a
**small** number of **larger conv filters**

- Keeping receptive field sizes large enough (∵ 작은 kernel size의 conv layer도 많이 쌓으면, receptive field 크기 大)
- Deeper with more non-linearities
- Fewer parameters

`Input`

- 224x224 RGB images (same with AlexNet)
- Subtracting mean RGB values of training images ⇒ normalize

<details>
<summary>📎subtracting RGB mean 하는 이유  </summary>
<div markdown="1">       

 - **Mean subtraction**은 가장 일반적인 전처리 형식이다. 이 방법은 데이터의 모든 개별*피쳐*에 **그 평균을 빼는 것**이다. 이에 대한 기하학적 해석은 데이터의 **모든 차원에 대한 분포의 중심을 원점으로 이동**하는 것이다.
- **Unnormalized**의 경우, 앞뒤로 왔다 갔다 하면서 수많은 단계를 거쳐 최적값에 도달하게 된다. 또한, 학습률(Learning Rate)를 작게 설정 해야 한다.
- 반면에, **Normalied**의 경우 어디서 시작하든 쉽게 최적값에 도달할 수 있으며, 학습률을 상대적으로 높여서 사용할 수 있게 때문에 **빠르게 훈련시킬 수 있다.**
- 참고 : [cs231n: Setting up the data and the model (tistory.com)](https://fabj.tistory.com/52)

</div>
</details>   

`Other details`

- **ReLU** for non-linearity
- No local response normalization (LRN X)

# Reference

### Course overview

- Thompson, Margaret Thatcher: A New Ilusion, Perception 1980
- Kirillov et al., Panoptic Segmentation, CVPR 2019
- Gordon et al., Depth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown Cameras, ICCV 2019
- Huang et al., Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization, ICCV 2017
- Selvaraju et al., Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, ICCV 2017

### CNN architectures for image classification 1

- Lecun et al., Gradient-based Learning Applied to Document Recognition, Proceedings of the IEEE 1998
- Krizhevsky et al., ImageNet Classification with Deep Convolutional Neural Networks, NIPS 2012
- Simonyan and Zisserman, Very Deep Convolutional Networks for Large-Scale Image Recognition, ICLR 2015
